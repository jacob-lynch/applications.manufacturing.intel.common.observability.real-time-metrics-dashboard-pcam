<QueryConfig AutoExecuteQueryOnLoad="false" PreventSQLBeautification="false">
  <ChangeLog>Changed by GER\mfg_rlouk on 2/4/2018 11:35:46 AM from F28PAP216N13.F28PROD.MFG.INTEL.COM using LogAnalyzer2 v2.9.61013.0546</ChangeLog>
  <ChangeLog>Changed by GER\mfg_rlouk on 2/4/2018 11:34:10 AM from F28PAP216N13.F28PROD.MFG.INTEL.COM using LogAnalyzer2 v2.9.61013.0546</ChangeLog>
  <ChangeLog>Changed by GER\mfg_rlouk on 2/4/2018 11:29:27 AM from F28PAP216N13.F28PROD.MFG.INTEL.COM using LogAnalyzer2 v2.9.61013.0546</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 9/30/2014 8:51:55 AM from RF3PTS216.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.8.40923.2310</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 11/12/2013 12:12:38 PM from RF3PTS216.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.7.31110.0013</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 5/1/2013 4:51:35 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30430.2016</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 2/4/2013 11:33:12 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30204.0928</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 2/4/2013 11:32:58 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30204.0928</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/30/2013 11:33:58 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30129.2054</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/30/2013 11:33:49 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30129.2054</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/29/2013 9:45:42 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30129.2054</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/29/2013 9:06:38 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30127.2007</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/29/2013 9:05:03 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30127.2007</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/29/2013 8:38:29 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30127.2007</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/28/2013 11:37:19 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30127.2007</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/28/2013 11:10:26 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30127.2007</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/24/2013 3:51:27 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/24/2013 3:38:28 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/24/2013 3:27:50 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/24/2013 10:38:32 AM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/23/2013 4:31:10 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/23/2013 4:00:04 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/23/2013 3:52:26 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/23/2013 3:51:58 PM from RF3PTS215.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.6.30123.0758</ChangeLog>
  <ChangeLog>Changed by AMR\mfg_dmdavies on 1/11/2013 8:40:24 AM from RF3PAP216N1.RF3PROD.MFG.INTEL.COM using LogAnalyzer2 v2.5.20326.1729</ChangeLog>
  <UNIQECredentials UserId="uber" Site="rf3sap110-alias.rf3stg.mfgint.intel.com" DataSource="D1D_STAG_LogAnalyzer" SaveCredentials="true" UseUNIQECredentialsOnStartUp="false" QueryTimeOutInSeconds="-1">
    <Password />
    <Name />
  </UNIQECredentials>
  <QueryAttributes>
    <OutputDateFormat>yyyy.ww</OutputDateFormat>
  </QueryAttributes>
  <PostQuerySQL>select * from %{OUTPUT}</PostQuerySQL>
  <TableLayoutConfig>
    <BoundColumnList>Entity,CE Code,life,Process,Building,Bay,Event Sub Type,SL1Finish,SL1Finish+WW,FinishDate,CapCode</BoundColumnList>
  </TableLayoutConfig>
  <RowHighlightingRules Enable="true" ColumnName="Globals" FilterRegex="O" RowColorString="NamedColor:White" />
  <RowHighlightingRules Enable="true" ColumnName="Platforms" FilterRegex="O" RowColorString="NamedColor:White" />
  <RowHighlightingRules Enable="true" ColumnName="AWIT" FilterRegex="O" RowColorString="NamedColor:White" />
  <RowHighlightingRules Enable="true" ColumnName="Pingable_WC" FilterRegex="Pingable" RowColorString="NamedColor:Lime" />
  <RowHighlightingRules Enable="true" ColumnName="Pingable_WC" FilterRegex="" RowColorString="NamedColor:White" />
  <TabOrder>G0,C0,G1,G2</TabOrder>
  <MainPivotConfig Enable="true">
    <QuerySQL>---[MAIN_CSV]---
BEGIN MAIN_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		*
	FROM
		%{output}
END MAIN_CSV
---[ACTUAL_CSV]---
BEGIN ACTUAL_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, Count(*) as TotalActual
	USING
		quantize(FinishDate, 604800) as WW
	FROM
		%{MAIN_CSV}
	WHERE
		capcode like 'PCAM' and FinishDate is not null
	GROUP BY
		WW
	ORDER BY
		WW
END ACTUAL_CSV
---[PLAN_CSV]---
BEGIN PLAN_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, Count(*) as TotalPlan
	USING
		quantize([SL1Finish+WW], 604800) as WW
	FROM
		%{MAIN_CSV}
	WHERE
		capcode like 'PCAM'
	GROUP BY
		WW
	ORDER BY
		WW
END PLAN_CSV
---[FILLP_CSV]---
BEGIN FILLP_CSV
	UseMethod QuantizeDateWithGaps with -- **Convert to time-scale with NULL values for missing time data**
		InputFile = "%{PLAN_CSV}" -- Input CSV File containing table data [String]
		DateColumn = "WW" -- Column containing the date/time [String]
		PivotColumns = "" -- Column containing the pivot key(s) (comma-separated if more than one) [String]
		TimeFilter = "last 30 minutes" -- Time filter specifying the range of date/time for which to look for missing data [e.g., Last 30 minutes, etc.] [String]
		ColumnsToSelect = "*" -- Columns to select from the input table for output (comma-separated) [String]
		TimeInterval = "1w" -- Time interval for which gaps are to be found (specify -1 to auto-determine) [e.g., 60s for 60 seconds, 2m for 2 mins, 3 h for 3 hrs, 1d for 1 day] [String]
		QuantizeDateColumn = false -- Quantize the DateColumn using TimeInterval and replace it's value [Boolean]
		MissingValueSubstituteForStringFields = "null" -- Replacement for missing value for string columns (specify "null" to populate with DBNull) [e.g., null, N/A] [String]
		MissingValueSubstituteForNumericFields = "0" -- Replacement for missing value for numeric columns (specify "null" to populate with DBNull) [e.g., null, 0, -1] [String]
END FILLP_CSV
---[FILLA_CSV]---
BEGIN FILLA_CSV
	UseMethod QuantizeDateWithGaps with -- **Convert to time-scale with NULL values for missing time data**
		InputFile = "%{ACTUAL_CSV}" -- Input CSV File containing table data [String]
		DateColumn = "WW" -- Column containing the date/time [String]
		PivotColumns = "" -- Column containing the pivot key(s) (comma-separated if more than one) [String]
		TimeFilter = "last 30 minutes" -- Time filter specifying the range of date/time for which to look for missing data [e.g., Last 30 minutes, etc.] [String]
		ColumnsToSelect = "*" -- Columns to select from the input table for output (comma-separated) [String]
		TimeInterval = "1w" -- Time interval for which gaps are to be found (specify -1 to auto-determine) [e.g., 60s for 60 seconds, 2m for 2 mins, 3 h for 3 hrs, 1d for 1 day] [String]
		QuantizeDateColumn = false -- Quantize the DateColumn using TimeInterval and replace it's value [Boolean]
		MissingValueSubstituteForStringFields = "null" -- Replacement for missing value for string columns (specify "null" to populate with DBNull) [e.g., null, N/A] [String]
		MissingValueSubstituteForNumericFields = "0" -- Replacement for missing value for numeric columns (specify "null" to populate with DBNull) [e.g., null, 0, -1] [String]
END FILLA_CSV
---[SORTA_CSV]---
BEGIN SORTA_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, TotalActual
	FROM
		%{FILLA_CSV}
	ORDER BY
		WW
END SORTA_CSV
---[SORTP_CSV]---
BEGIN SORTP_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, TotalPlan
	FROM
		%{FILLP_CSV}
	ORDER BY
		WW
END SORTP_CSV
---[SUM_CSV]---
BEGIN SUM_CSV
	UseMethod CalculateRunningSum with -- **Get Running Sum for a Column in the Table**
		InputFile = "%{SORTA_CSV}" -- Input CSV file [String]
		ColumnToCalculateSumFor = "TotalActual" -- Input CSV file [String]
		PivotColumn = "" -- Pivot column (optional) [String]
		NewColumnForRunningSum = "Actual" -- New column to be added with running sum [String]
		NewColumnForAbsoluteRunningSum = "AbsoluteRunningTotalActual" -- New column to be added with absolute running sum which ignores the pivot column [String]
END SUM_CSV
---[SUMPLAN_CSV]---
BEGIN SUMPLAN_CSV
	UseMethod CalculateRunningSum with -- **Get Running Sum for a Column in the Table**
		InputFile = "%{SORTP_CSV}" -- Input CSV file [String]
		ColumnToCalculateSumFor = "TotalPlan" -- Input CSV file [String]
		PivotColumn = "" -- Pivot column (optional) [String]
		NewColumnForRunningSum = "Planned" -- New column to be added with running sum [String]
		NewColumnForAbsoluteRunningSum = "AbsoluteRunningPlan" -- New column to be added with absolute running sum which ignores the pivot column [String]
END SUMPLAN_CSV
---[JOIN_CSV]---
BEGIN JOIN_CSV
	UseMethod Join with -- **Do an outer join for two tables based on a matching column**
		InputFile1 = "%{SUM_CSV}" -- Input CSV File #1 containing table data [String]
		InputFile2 = "%{SUMPLAN_CSV}" -- Input CSV File #2 containing table data [String]
		JoinColumn = "WW" -- Column name(s) on which to join the two tables (more than one column can be supplied by using comma as a separator) [String]
		IgnoreDuplicate = false -- Flag to ignore duplicate items in the JoinColumn [Boolean]
		IgnoreCase = false -- Flag to ignore case in the JoinColumn [Boolean]
		OnlyShowNonMatchingRows = false -- Flag to only show non-matching orphan rows [Boolean]
		IncludeNonMatchingRows = true -- Flag to also include non-matching orphan rows (but with blank entries) [Boolean]
		MustIncludeOutputColumns = "" -- Name of columns (comma-separated) that must be included in output even if the join does not materialize [String]
END JOIN_CSV
---Begin Main Query---
Using CSV with
SELECT
	top 10
	WW,
	Actual,
	Planned
FROM
	%{JOIN_CSV}
WHERE
	WW &lt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) )
ORDER BY
	WW desc
</QuerySQL>
    <ColumnX FieldName="WW" FieldType="String" SortMode="Default" SortOrder="Ascending" SummaryType="Sum" SortBySummaryInfo="false" />
    <ColumnY FieldName="" FieldType="" SortMode="Default" SortOrder="Ascending" SummaryType="Count" SortBySummaryInfo="false" />
    <ColumnData FieldName="Actual, Planned" FieldType="Int32" SortMode="Default" SortOrder="Ascending" SummaryType="Average,Average" SortBySummaryInfo="false" />
    <ChartType>Line</ChartType>
    <Name>SC Completed</Name>
    <ChartTitle>SC Completed
Planned vs. Actual</ChartTitle>
    <ColorPalette>Median</ColorPalette>
    <ChartExportSize>800x400</ChartExportSize>
    <TimeIntervalForDateScale>7d</TimeIntervalForDateScale>
    <DateFormat>Auto</DateFormat>
    <CustomPalette />
    <TableData />
  </MainPivotConfig>
  <OutputDataGridConfig Name="SC">
    <SQL>---[MAIN_CSV]---
BEGIN MAIN_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		*
	FROM
		%{output}
END MAIN_CSV
---[ACTUAL_CSV]---
BEGIN ACTUAL_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, Count(*) as TotalActual
	USING
		quantize(FinishDate, 604800) as WW
	FROM
		%{MAIN_CSV}
	WHERE
		capcode like 'PCAM' and FinishDate is not null
	GROUP BY
		WW
	ORDER BY
		WW
END ACTUAL_CSV
---[PLAN_CSV]---
BEGIN PLAN_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, Count(*) as TotalPlan
	USING
		quantize([SL1Finish+WW], 604800) as WW
	FROM
		%{MAIN_CSV}
	WHERE
		capcode like 'PCAM'
	GROUP BY
		WW
	ORDER BY
		WW
END PLAN_CSV
---[FILLP_CSV]---
BEGIN FILLP_CSV
	UseMethod QuantizeDateWithGaps with -- **Convert to time-scale with NULL values for missing time data**
		InputFile = "%{PLAN_CSV}" -- Input CSV File containing table data [String]
		DateColumn = "WW" -- Column containing the date/time [String]
		PivotColumns = "" -- Column containing the pivot key(s) (comma-separated if more than one) [String]
		ColumnsToSelect = "*" -- Columns to select from the input table for output (comma-separated) [String]
		TimeInterval = "1w" -- Time interval for which gaps are to be found (specify -1 to auto-determine) [e.g., 60s for 60 seconds, 2m for 2 mins, 3 h for 3 hrs, 1d for 1 day] [String]
		QuantizeDateColumn = false -- Quantize the DateColumn using TimeInterval and replace it's value [Boolean]
		MissingValueSubstituteForStringFields = "null" -- Replacement for missing value for string columns (specify "null" to populate with DBNull) [e.g., null, N/A] [String]
		MissingValueSubstituteForNumericFields = "0" -- Replacement for missing value for numeric columns (specify "null" to populate with DBNull) [e.g., null, 0, -1] [String]
END FILLP_CSV
---[FILLA_CSV]---
BEGIN FILLA_CSV
	UseMethod QuantizeDateWithGaps with -- **Convert to time-scale with NULL values for missing time data**
		InputFile = "%{ACTUAL_CSV}" -- Input CSV File containing table data [String]
		DateColumn = "WW" -- Column containing the date/time [String]
		PivotColumns = "" -- Column containing the pivot key(s) (comma-separated if more than one) [String]
		ColumnsToSelect = "*" -- Columns to select from the input table for output (comma-separated) [String]
		TimeInterval = "1w" -- Time interval for which gaps are to be found (specify -1 to auto-determine) [e.g., 60s for 60 seconds, 2m for 2 mins, 3 h for 3 hrs, 1d for 1 day] [String]
		QuantizeDateColumn = false -- Quantize the DateColumn using TimeInterval and replace it's value [Boolean]
		MissingValueSubstituteForStringFields = "null" -- Replacement for missing value for string columns (specify "null" to populate with DBNull) [e.g., null, N/A] [String]
		MissingValueSubstituteForNumericFields = "0" -- Replacement for missing value for numeric columns (specify "null" to populate with DBNull) [e.g., null, 0, -1] [String]
END FILLA_CSV
---[SORTA_CSV]---
BEGIN SORTA_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, TotalActual
	FROM
		%{FILLA_CSV}
	ORDER BY
		WW
END SORTA_CSV
---[SORTP_CSV]---
BEGIN SORTP_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		WW, TotalPlan
	FROM
		%{FILLP_CSV}
	ORDER BY
		WW
END SORTP_CSV
---[SUM_CSV]---
BEGIN SUM_CSV
	UseMethod CalculateRunningSum with -- **Get Running Sum for a Column in the Table**
		InputFile = "%{SORTA_CSV}" -- Input CSV file [String]
		ColumnToCalculateSumFor = "TotalActual" -- Input CSV file [String]
		PivotColumn = "" -- Pivot column (optional) [String]
		NewColumnForRunningSum = "Actual" -- New column to be added with running sum [String]
		NewColumnForAbsoluteRunningSum = "AbsoluteRunningTotalActual" -- New column to be added with absolute running sum which ignores the pivot column [String]
END SUM_CSV
---[SUMPLAN_CSV]---
BEGIN SUMPLAN_CSV
	UseMethod CalculateRunningSum with -- **Get Running Sum for a Column in the Table**
		InputFile = "%{SORTP_CSV}" -- Input CSV file [String]
		ColumnToCalculateSumFor = "TotalPlan" -- Input CSV file [String]
		PivotColumn = "" -- Pivot column (optional) [String]
		NewColumnForRunningSum = "Planned" -- New column to be added with running sum [String]
		NewColumnForAbsoluteRunningSum = "AbsoluteRunningPlan" -- New column to be added with absolute running sum which ignores the pivot column [String]
END SUMPLAN_CSV
---[JOIN_CSV]---
BEGIN JOIN_CSV
	UseMethod Join with -- **Do an outer join for two tables based on a matching column**
		InputFile1 = "%{SUM_CSV}" -- Input CSV File #1 containing table data [String]
		InputFile2 = "%{SUMPLAN_CSV}" -- Input CSV File #2 containing table data [String]
		JoinColumn = "WW" -- Column name(s) on which to join the two tables (more than one column can be supplied by using comma as a separator) [String]
		IgnoreDuplicate = false -- Flag to ignore duplicate items in the JoinColumn [Boolean]
		IgnoreCase = false -- Flag to ignore case in the JoinColumn [Boolean]
		OnlyShowNonMatchingRows = false -- Flag to only show non-matching orphan rows [Boolean]
		IncludeNonMatchingRows = true -- Flag to also include non-matching orphan rows (but with blank entries) [Boolean]
		MustIncludeOutputColumns = "" -- Name of columns (comma-separated) that must be included in output even if the join does not materialize [String]
END JOIN_CSV
---Begin Main Query---
Using CSV with
SELECT
	top 10
	WW,
	Actual,
	Planned
FROM
	%{JOIN_CSV}
WHERE
	WW &lt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) )
ORDER BY
	WW desc
</SQL>
    <TableLayoutConfig>
      <BoundColumnList>WW,Actual,Planned</BoundColumnList>
    </TableLayoutConfig>
    <TableSchema><xs:schema id="NewDataSet" xmlns="" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
  <xs:element name="NewDataSet" msdata:IsDataSet="true" msdata:MainDataTable="Results" msdata:UseCurrentLocale="true">
    <xs:complexType>
      <xs:choice minOccurs="0" maxOccurs="unbounded">
        <xs:element name="Results">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="WW" type="xs:dateTime" minOccurs="0" />
              <xs:element name="Actual" type="xs:int" minOccurs="0" />
              <xs:element name="Planned" type="xs:int" minOccurs="0" />
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:choice>
    </xs:complexType>
  </xs:element>
</xs:schema>
</TableSchema>
    <TableData />
  </OutputDataGridConfig>
  <OutputDataGridConfig Name="Issues Report">
    <SQL>----------&lt;VARIABLES&gt;------------
var TIMEFILTER = ""
var SQL_NODE = "@{TIPRSQLNODE}"
var CATALOG = "DSI"
var TIPRPW = "@{TIPRPW}"
var TIPRID = "@{TIPRID}"
---[ISSUES_CSV]---
BEGIN ISSUES_CSV
	UseMethod QueryOLEDB with -- **Run an OLEDB query**
		ConnectionString = "Provider=SQLOLEDB.1;User ID=${TIPRID};Password=${TIPRPW};Persist Security Info=True;Data Source=${SQL_NODE}; Initial Catalog=${CATALOG};" -- OLEDB connection string [String]
		Query = "SELECT * FROM dbo.SetupIssues WHERE Status = 'Open' " -- SQL Query to run against OLEDB provider [String]
END ISSUES_CSV
---Begin Main Query---
Using CSV with
SELECT
	Entity,
	DateofUpdate as Date,
	GatingItem as [Gating Item],
	Owner,
	Issue,
	PlanofAction as [Action Plan]
FROM
	%{ISSUES_CSV}
ORDER BY
	Date desc
</SQL>
    <TableLayoutConfig>
      <BoundColumnList>Entity,Date,Gating Item,Owner,Issue,Action Plan</BoundColumnList>
    </TableLayoutConfig>
    <TableSchema><xs:schema id="NewDataSet" xmlns="" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
  <xs:element name="NewDataSet" msdata:IsDataSet="true" msdata:MainDataTable="Results" msdata:UseCurrentLocale="true">
    <xs:complexType>
      <xs:choice minOccurs="0" maxOccurs="unbounded">
        <xs:element name="Results">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="Entity" type="xs:string" minOccurs="0" />
              <xs:element name="Date" type="xs:dateTime" minOccurs="0" />
              <xs:element name="Gating_x0020_Item" type="xs:string" minOccurs="0" />
              <xs:element name="Owner" type="xs:string" minOccurs="0" />
              <xs:element name="Issue" type="xs:string" minOccurs="0" />
              <xs:element name="Action_x0020_Plan" type="xs:string" minOccurs="0" />
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:choice>
    </xs:complexType>
  </xs:element>
</xs:schema>
</TableSchema>
    <TableData />
  </OutputDataGridConfig>
  <OutputDataGridConfig Name="Details">
    <SQL>---[MAIN_CSV]---
BEGIN MAIN_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		*
	FROM
		%{OUTPUT}
END MAIN_CSV
---[LATE_CSV]---
BEGIN LATE_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		'Late' as Status,
		Entity as SCCompleted
	FROM
		%{MAIN_CSV}
	WHERE
		CapCode = 'PCAM' and
		[SL1Finish+WW] &lt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) )
		and ((FinishDate is null) or ((FinishDate &gt; [SL1Finish+WW]) and FinishDate &gt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) )))
END LATE_CSV
---[LATETRAN_CSV]---
BEGIN LATETRAN_CSV
	UseMethod MergeRows with -- **Merge consecutive rows in a given table based on matching column values**
		InputFiles = "%{LATE_CSV}" -- Input files (comma-separated) containing table data [String]
		MergeKeyColumn = "Status" -- Columns (comma-separated) to use for comparing values between consecutive rows before merging [String]
		ColumnsToMerge = "SCCompleted" -- Columns (comma-separated) whose values will be merged [String]
		LineSeparator = ", " -- Line separator to use when merging consecutive consecutive rows [String]
		MaxRowsToMerge = -1 -- Maximum number of rows to merge [Int32]
		MaxCellSizeInBytes = 100000 -- Maximum size of merged cell in bytes [Int32]
END LATETRAN_CSV
---[EARLY_CSV]---
BEGIN EARLY_CSV
	---Begin Main Query---
	Using CSV with
	SELECT
		'Early' as Status,
		Entity as SCCompleted
	FROM
		%{MAIN_CSV}
	WHERE
		CapCode = 'PCAM' and
		FinishDate is not null and
		[SL1Finish+WW] &gt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) ) and
		FinishDate &lt; SUB(TO_LOCALTIME(SYSTEM_TIMESTAMP()), TIMESTAMP( '7', 'd' ) )
END EARLY_CSV
---[EARLYTRAN_CSV]---
BEGIN EARLYTRAN_CSV
	UseMethod MergeRows with -- **Merge consecutive rows in a given table based on matching column values**
		InputFiles = "%{EARLY_CSV}" -- Input files (comma-separated) containing table data [String]
		MergeKeyColumn = "Status" -- Columns (comma-separated) to use for comparing values between consecutive rows before merging [String]
		ColumnsToMerge = "SCCompleted" -- Columns (comma-separated) whose values will be merged [String]
		LineSeparator = ", " -- Line separator to use when merging consecutive consecutive rows [String]
		MaxRowsToMerge = -1 -- Maximum number of rows to merge [Int32]
		MaxCellSizeInBytes = 100000 -- Maximum size of merged cell in bytes [Int32]
END EARLYTRAN_CSV
---[JOIN_CSV]---
BEGIN JOIN_CSV
	UseMethod MergeTables with -- **Merge content of two tables by doing an outer join on the column schema (no join done here)**
		InputFile1 = "%{LATETRAN_CSV}" -- Input CSV File #1 containing table data [String]
		InputFile2 = "%{EARLYTRAN_CSV}" -- Input CSV File #2 containing table data [String]
		AutoDetectColumnTypes = false -- Auto-detect column types before merging [Boolean]
END JOIN_CSV
---Begin Main Query---
Using CSV with
SELECT
	*
FROM
	%{JOIN_CSV}
</SQL>
    <TableLayoutConfig>
      <BoundColumnList>Status,SCCompleted</BoundColumnList>
    </TableLayoutConfig>
    <TableSchema><xs:schema id="NewDataSet" xmlns="" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
  <xs:element name="NewDataSet" msdata:IsDataSet="true" msdata:MainDataTable="Results" msdata:UseCurrentLocale="true">
    <xs:complexType>
      <xs:choice minOccurs="0" maxOccurs="unbounded">
        <xs:element name="Results">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="Status" type="xs:string" minOccurs="0" />
              <xs:element name="SCCompleted" type="xs:string" minOccurs="0" />
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:choice>
    </xs:complexType>
  </xs:element>
</xs:schema>
</TableSchema>
    <TableData />
  </OutputDataGridConfig>
  <AutoPivotConfig OutputDateFormat="yyyy ww.w hh tt" />
  <MailConfig>
    <PostProcessingSQL Enable="false" />
    <EmailCondition Enable="false" Operator="&gt;" ThresholdRowCount="0">
      <SQLQuery />
      <RunScriptBeforeEmailing>false</RunScriptBeforeEmailing>
      <Script />
    </EmailCondition>
    <PostProcessingCondition Enable="false" Operator="&gt;" ThresholdRowCount="0">
      <SQLQuery />
      <RunScriptBeforeEmailing>false</RunScriptBeforeEmailing>
    </PostProcessingCondition>
    <CopyCondition Enable="false" Operator="&gt;" ThresholdRowCount="0">
      <SQLQuery />
      <RunScriptBeforeEmailing>false</RunScriptBeforeEmailing>
    </CopyCondition>
    <SMTPServer>smtp.intel.com</SMTPServer>
    <From>LogAnalyzer2@intel.com</From>
    <ReplyTo>@{DASHBOARD_OWNER_ALERT_DIST}</ReplyTo>
    <To />
    <Subject>[${DOMAIN}] TIPR_Metrics</Subject>
    <Body />
    <MailAttachments FileNamingFormat="${SETTINGS}" Excel="false" CSV="false" LAS="false" LAD="false" Exceptions="false" StatusLogs="false" PivotChart="true" ZipAttachments="false" DoNotSendAttachmentsWithEmail="false" IncludeJobStatisticsHeader="false" IncludeQuerySettingsFileHeader="false" IncludeVariableDefinitionHeader="false" IncludeRowCountInSubject="false" />
    <IncludeSQLQueryInBody>false</IncludeSQLQueryInBody>
    <TableInBody Enable="true">
      <SQL />
    </TableInBody>
    <CopyOutputToDirectory>true</CopyOutputToDirectory>
    <OutputDirectory>@{DASHBOARD_WEB_SPOOL}\PCAM\Output</OutputDirectory>
    <IncludeAutoPivotSummary>false</IncludeAutoPivotSummary>
    <IncludeCustomPivotSummary>false</IncludeCustomPivotSummary>
    <IgnoreExceptions>false</IgnoreExceptions>
    <OnlyCopyOutputIfEmailConditionIsMet>false</OnlyCopyOutputIfEmailConditionIsMet>
  </MailConfig>
  <TableSchema><xs:schema id="NewDataSet" xmlns="" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
  <xs:element name="NewDataSet" msdata:IsDataSet="true" msdata:MainDataTable="Results" msdata:UseCurrentLocale="true">
    <xs:complexType>
      <xs:choice minOccurs="0" maxOccurs="unbounded">
        <xs:element name="Results">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="Entity" type="xs:string" minOccurs="0" />
              <xs:element name="CE_x0020_Code" type="xs:string" minOccurs="0" />
              <xs:element name="life" type="xs:int" minOccurs="0" />
              <xs:element name="Process" type="xs:int" minOccurs="0" />
              <xs:element name="Building" type="xs:string" minOccurs="0" />
              <xs:element name="Bay" type="xs:string" minOccurs="0" />
              <xs:element name="Event_x0020_Sub_x0020_Type" type="xs:string" minOccurs="0" />
              <xs:element name="SL1Finish" type="xs:dateTime" minOccurs="0" />
              <xs:element name="SL1Finish_x002B_WW" type="xs:dateTime" minOccurs="0" />
              <xs:element name="FinishDate" type="xs:dateTime" minOccurs="0" />
              <xs:element name="CapCode" type="xs:string" minOccurs="0" />
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:choice>
    </xs:complexType>
  </xs:element>
</xs:schema>
</TableSchema>
  <TableData />
  <LastRunAbsoluteDateFilter />
  <CustomQueryMethodScripts>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.IO;
using System.Text;
using System.Diagnostics;
using System.Text.RegularExpressions;
using Intel.LogAnalyzer;
using Intel.LogAnalyzer.Common;
using Intel.LogAnalyzer.Configuration;
using Intel.LogAnalyzer.Utility;

namespace Intel.LogAnalyzer.BuiltInQueryMethods_Sample
{
    public class CalculateRunningSum : CustomQueryMethodBase
    {

        /// &lt;summary&gt;
        /// Test harness
        /// &lt;/summary&gt;
        [STAThread]
        private static void Main()
        {
            new CalculateRunningSum
            {
                InputFile = @"D:\Temp\test.csv",
                ColumnToCalculateSumFor = "Total",
                PivotColumn = "MetSLA",
                NewColumnForRunningSum = "RunningTotal",
                NewColumnForAbsoluteRunningSum = "AbsoluteRunningTotal"
            }.Test();
        }

        // Output type is QueryResult
        public override CustomQueryOutputType OutputType { get { return CustomQueryOutputType.QueryResult; } }

        public override string Description
        {
            get { return "Get Running Sum for a Column in the Table"; }
        }

        [Description("Input CSV file")]
        public string InputFile { get; set; }
        
        [Description("Input CSV file")]
        public string ColumnToCalculateSumFor { get; set; }
        
        [Description("Pivot column (optional)")]
        [DefaultValue("")]
        public string PivotColumn { get; set; }

        [Description("New column to be added with running sum")]
        [DefaultValue("RunningSum")]
        public string NewColumnForRunningSum { get; set; }
        
        [Description("New column to be added with absolute running sum which ignores the pivot column")]
        [DefaultValue("")]
        public string NewColumnForAbsoluteRunningSum { get; set; }

        public override QueryResult GetQueryResult()
        {
            DataTable table = UtilityMethods.ConvertCSVFileToDataTable(InputFile, true);
            int colIndex = GetColumnIndex(table, ColumnToCalculateSumFor, true);
            int pivotIndex = -1;

            if (PivotColumn.Trim().Length &gt; 0)
            {
                pivotIndex = GetColumnIndex(table, PivotColumn, true);
            }

            table.Columns.Add(NewColumnForRunningSum, typeof(double));
            int newColIndex = GetColumnIndex(table, NewColumnForRunningSum);

            NewColumnForAbsoluteRunningSum = NewColumnForAbsoluteRunningSum.Trim();
            int newAbsColIndex = -1;
            if (NewColumnForAbsoluteRunningSum.Length &gt; 0)
            {
                table.Columns.Add(NewColumnForAbsoluteRunningSum, typeof(double));
                newAbsColIndex = GetColumnIndex(table, NewColumnForAbsoluteRunningSum);
            }

            Dictionary&lt;string, double&gt; dict = new Dictionary&lt;string, double&gt;();
            double absoluteSum = 0.0;

            foreach (DataRow row in table.Rows)
            {
                string key = string.Empty;
                if (pivotIndex &gt;= 0)
                {
                    key = row[pivotIndex].ToStringEx();
                }
                double value = Convert.ToDouble(row[colIndex]);
                absoluteSum += value;
                double sum;
                if (dict.TryGetValue(key, out sum))
                {
                    sum += value;
                    dict[key] = sum;
                }
                else
                {
                    sum = value;
                    dict.Add(key, sum);
                }
                row[newColIndex] = sum;
                if (newAbsColIndex &gt;= 0)
                {
                    row[newAbsColIndex] = absoluteSum;
                }
            }
           
            return new QueryResult(table);
        }

    }
}</CustomQueryMethodScripts>
  <PivotGroupingSettings Enable="true" PivotColumn="IQ_WS">
    <SortyByColumn>Count</SortyByColumn>
  </PivotGroupingSettings>
  <QuerySQL><![CDATA[
--[FileName:"T:\Dashboard\Jobs\PCAM\1-hr\TIPR_Metrics.lasx"]------
----------<VARIABLES>------------
var NODEFILTER = "*"
var DOMAIN = "F28PROD"
var TIPRPW = "@{TIPRPW}"
var TIPRID = "@{TIPRID}"
var TIMEFILTER = ""
var SQL_NODE = "@{TIPRSQLNODE}"
var CATALOG = "DSI"
---[OLEDB_CSV]---
BEGIN OLEDB_CSV
	---Begin Main Query---
	Using SQLDB with
		HostName = "${SQL_NODE}" -- SQL host name to connect to
		Database = "${CATALOG}" -- Database/catalog name
		UserID = "${TIPRID}" -- User ID to use for connection (leave blank or specify / for IWA)
		Password = "${TIPRPW}" -- Password corresponding to the UserID
	SELECT DISTINCT c.Entity,
		p.[CE Code],
		p.life,
		p.Process,
		p.Building,
		p.Bay,
		p.[Event Sub Type],
		p.[SL1 Finish] AS SL1FinishTmp,
		c.FinishDate AS FinishDateTmp,
		c.CapCode
	FROM dbo.EntityCapabilities AS c
	INNER JOIN dbo.Primavera AS p ON (
			c.Entity = p.[Entity Code]
			AND c.Life = p.Life
			)
		AND (
			CapCode = 'PCAM'
			OR CapCode = 'WC'
			)
	WHERE p.[SL1 Finish] < getdate() + 30
		AND p.[SL1 Finish] > getdate() - 365
		AND (p.[Event Sub Type] = 'Install')
	ORDER BY p.[SL1 Finish]
END OLEDB_CSV
---Begin Main Query---
Using CSV with
SELECT
	Entity,
	[CE Code],
	life,
	Process,
	Building,
	Bay,
	[Event Sub Type],
	SL1Finish,
	[SL1Finish+WW],
	FinishDate,
	CapCode
USING
	quantize(SL1FinishTmp, 604800) as SL1Finish,
	add(SL1Finish, TIMESTAMP('0000-01-08', 'yyyy-MM-dd') ) as [SL1Finish+WW],
	quantize(FinishDateTmp, 604800) as FinishDate
FROM
	%{OLEDB_CSV}
WHERE
	Life = '1'
ORDER BY
	SL1Finish, entity

]]></QuerySQL>
</QueryConfig>